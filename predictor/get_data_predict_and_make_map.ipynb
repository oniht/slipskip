{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This part adds the path c:\\slipskip (or other directory where the repo is located) to the sys.path. \n",
    "# This is done so that blocks below can find the modules in other folders (eg. predictor.get_obs_from_api)\n",
    "import sys\n",
    "import os\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def rename_cols(df):\n",
    "    df = df.rename(columns={\"Precipitation amount\": \"rainfall_mm\", \"Snow depth\": \"snow_depth_cm\", \"Air temperature\": \"air_temp\", \n",
    "     \"Maximum temperature\": \"max_temp\", \"Minimum temperature\": \"min_temp\", \"Ground minimum temperature\": \"min_ground_temp\"})\n",
    "    cols = ['city','rainfall_mm', 'snow_depth_cm', 'air_temp', 'max_temp', 'min_temp', 'min_ground_temp']\n",
    "    df = df[cols]\n",
    "\n",
    "    return df\n",
    "\n",
    "def merge_and_replace(df, dfh):\n",
    "    # This functions makes a union of the daily and hourly observations and replaces some values\n",
    "    merged = pd.concat([df, dfh])\n",
    "    merged2 = merged.mask(merged == \"-\", np.nan)\n",
    "    merged2[[\"rainfall_mm\", \"snow_depth_cm\", \"air_temp\", \"max_temp\", \"min_temp\", \"min_ground_temp\"]] = merged2[[\"rainfall_mm\", \"snow_depth_cm\", \"air_temp\", \"max_temp\", \"min_temp\", \"min_ground_temp\"]].astype('float64')\n",
    "    merged2 = merged2[[\"city\",\"rainfall_mm\", \"snow_depth_cm\", \"air_temp\", \"max_temp\", \"min_temp\", \"min_ground_temp\"]].interpolate(axis=0)\n",
    "    merge3 = merged2\n",
    "    merge3[\"rainfall_mm\"] = merge3[\"rainfall_mm\"].where(merge3[\"rainfall_mm\"] >= 0, 0)\n",
    "    merge3[\"snow_depth_cm\"] = merge3[\"snow_depth_cm\"].mask(merge3[\"snow_depth_cm\"] == -1, 0)\n",
    "    return merge3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys, os\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "import datetime as dt\n",
    "from fmiopendata.wfs import download_stored_query\n",
    "from fmiopendata.utils import read_url\n",
    "#import xml.etree.ElementTree as ET\n",
    "from lxml import etree\n",
    "\n",
    "def enablePrint():\n",
    "    sys.stdout = sys.__stdout__\n",
    "\n",
    "def get_places():\n",
    "\n",
    "    xml = read_url(\"http://opendata.fmi.fi/wfs?service=WFS&version=2.0.0&request=getFeature&storedquery_id=fmi::ef::stations\")\n",
    "    #stree = etree.parse(xml)\n",
    "    #root = ET.fromstring(xml)\n",
    "    root = etree.fromstring(xml)\n",
    "    locations = dict()\n",
    "    for name in root.findall('.//{http://www.opengis.net/gml/3.2}name'):\n",
    "        if name.attrib['codeSpace'] == 'http://xml.fmi.fi/namespace/locationcode/name':\n",
    "            city = name.text.split(' ', 1)[0]\n",
    "            id = name.getparent().find('.//{http://www.opengis.net/gml/3.2}identifier').text\n",
    "            try:\n",
    "                place = name.text.split(' ', 1)[1]\n",
    "            except:\n",
    "                place = ''\n",
    "            if city in locations:\n",
    "                locations[city].append((place,id))\n",
    "            else:\n",
    "                locations[city] = [(place, id)]\n",
    "\n",
    "    #res = np.empty([1, 2])\n",
    "\n",
    "    #for c in cities:\n",
    "    #    x = np.array(locations[c]).reshape(len(locations[c]),1)\n",
    "    #    y = np.broadcast_to([c], (len(locations[c]),1))\n",
    "    #    r = np.hstack((x,y))\n",
    "    #    res = np.vstack((res,r))\n",
    "\n",
    "    return locations #res[1:,:]\n",
    "\n",
    "    \n",
    "\n",
    "def get_daily_obs(cities, places):\n",
    "\n",
    "    # Retrieve the last 10 days daily observations + todays latest 10h observation\n",
    "    end_time = dt.datetime.utcnow() - dt.timedelta(days=1)\n",
    "    start_time = end_time - dt.timedelta(days=10)\n",
    "    # Convert times to properly formatted strings\n",
    "    start_time = start_time.isoformat(timespec=\"seconds\") + \"Z\"\n",
    "    end_time = end_time.isoformat(timespec=\"seconds\") + \"Z\"\n",
    "\n",
    "    df = pd.DataFrame({'city': [],'Precipitation amount' : [], 'Air temperature' : [], 'Snow depth' : [], 'Minimum temperature' : [], 'Maximum temperature' : [], 'Ground minimum temperature' : [],})\n",
    "\n",
    "    for c in cities:\n",
    "        plcs = places[c]\n",
    "        appended_data = []\n",
    "        for p in plcs:\n",
    "        # For last 10d we get daily values\n",
    "            obs = download_stored_query(\"fmi::observations::weather::daily::multipointcoverage\",\n",
    "                                args=[\"fmisid=\" + p[1],\n",
    "                                    \"starttime=\" + start_time,\n",
    "                                    \"endtime=\" + end_time])\n",
    "            df2 = pd.DataFrame.from_dict({(i): obs.data[i][j]\n",
    "                                for i in obs.data.keys() \n",
    "                                for j in obs.data[i].keys()},\n",
    "                            orient='index')\n",
    "            df2 = df2.applymap(lambda x: x.get('value'))\n",
    "            if df2.empty == False and (df2['Air temperature'].isnull().all() == False or df2['Ground minimum temperature'].isnull().all() == False):\n",
    "                appended_data.append(df2)\n",
    "        df2 = pd.concat(appended_data)\n",
    "        df2['city'] = c\n",
    "        df2.index = df2.index.date\n",
    "        #df2 = df2.groupby([df2.index]).mean()\n",
    "        df = pd.concat([df,df2])\n",
    "    \n",
    "    df = df.groupby([df.index, \"city\"]).mean().reset_index(level=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "def get_hourly_obs(cities, places, cols):\n",
    "    # Retrieve the last 10 days daily observations + todays latest 10h observation\n",
    "    end_time = dt.datetime.utcnow()\n",
    "    start_time = end_time - dt.timedelta(hours=10)\n",
    "    # Convert times to properly formatted strings\n",
    "    start_time = start_time.isoformat(timespec=\"seconds\") + \"Z\"\n",
    "    end_time = end_time.isoformat(timespec=\"seconds\") + \"Z\"\n",
    "\n",
    "    dfh = pd.DataFrame({'city': [], 'Air temperature' : [], 'Precipitation amount' : [],'Snow depth' : []})\n",
    "\n",
    "    for c in cities:\n",
    "        plcs = places[c]\n",
    "        appended_data = []\n",
    "        for p in plcs:\n",
    "        # For last 10h we get all observations and use these to calculate the \"daily\" observations for today\n",
    "            obs = download_stored_query(\"fmi::observations::weather::multipointcoverage\",\n",
    "                                args=[\"fmisid=\" + p[1],\n",
    "                                    \"starttime=\" + start_time,\n",
    "                                    \"endtime=\" + end_time])\n",
    "            dfh2 = pd.DataFrame.from_dict({(i): obs.data[i][j]\n",
    "                                for i in obs.data.keys() \n",
    "                                for j in obs.data[i].keys()},\n",
    "                            orient='index')\n",
    "            dfh2 = dfh2.applymap(lambda x: x.get('value'))\n",
    "            # We have to calculate some new columns for these hourly observations\n",
    "            if dfh2.empty == False:\n",
    "                dfh2['Minimum temperature'] = dfh2['Air temperature'].min()\n",
    "                dfh2['Maximum temperature'] = dfh2['Air temperature'].max()\n",
    "                dfh2['Air temperature'] = dfh2['Air temperature'].mean()\n",
    "                dfh2['Ground minimum temperature'] = dfh2['Minimum temperature']\n",
    "                dfh2['Precipitation amount'] = dfh2['Precipitation amount'].sum()\n",
    "            # Let's take only last calculated value as daily value for station\n",
    "            dfh2 = dfh2.sort_index(axis=0, ascending=False).head(1)\n",
    "            if dfh2.empty == False and dfh2['Air temperature'].isnull().all() == False:\n",
    "                appended_data.append(dfh2)\n",
    "        dfh2 = pd.concat(appended_data)\n",
    "        dfh2['city'] = c\n",
    "        dfh2.index = dfh2.index.date\n",
    "        dfh2 = dfh2[cols]\n",
    "        dfh = pd.concat([dfh,dfh2])\n",
    "    \n",
    "    # Let's take average of station for that city\n",
    "    dfh = dfh.groupby([dfh.index, \"city\"]).mean().reset_index(level=1)\n",
    "    \n",
    "    return dfh\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cities = ['Helsinki', 'Kuopio', 'Jyväskylä', 'Lahti', 'Oulu']\n",
    "\n",
    "# places = get_places()\n",
    "# df = get_daily_obs(cities, places)\n",
    "# # dfh = get_hourly_obs(cities, places, df.columns)\n",
    "\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No observations found\n",
      "No observations found\n",
      "No observations found\n",
      "No observations found\n"
     ]
    }
   ],
   "source": [
    "# This part executes all the data loading from APIs (get_obs_from_api.py) and does the preprocessing for data (weather_data_trimming.py).\n",
    "# It takes in a list of cities and starts by searching all the observation stations in theses cities. \n",
    "# It then downloads the observations from these stations and basically takes an average over all the stations in one city.\n",
    "# OBS! When excuting it prints quite a lot of \"No observations found\" because some of these stations are inactive. Don't worry about that.\n",
    "import importlib\n",
    "# import numpy as np\n",
    "# import data.weather_data.weather_data_trimming as f\n",
    "# import predictor.get_obs_from_api as g\n",
    "# importlib.reload(f)\n",
    "# importlib.reload(g)\n",
    "\n",
    "cities = ['Helsinki', 'Kuopio', 'Jyväskylä', 'Lahti', 'Oulu']\n",
    "\n",
    "places = get_places()\n",
    "df = get_daily_obs(cities, places)\n",
    "dfh = get_hourly_obs(cities, places, df.columns)\n",
    "\n",
    "df2 = rename_cols(df)\n",
    "dfh2 = rename_cols(dfh)\n",
    "df2 = merge_and_replace(df2, dfh2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "def get_features(df, cities):\n",
    "\n",
    "    appended_data = []\n",
    "    for c in cities:\n",
    "        dff = df.loc[df['city'] == c]\n",
    "        dff[\"rain_sum_7d\"] = dff[\"rainfall_mm\"].replace(-1, 0).shift(1).rolling(7).sum().round(2)\n",
    "        dff[\"snow_depth_cm\"] = dff[\"snow_depth_cm\"].replace(0, np.nan).ffill()\n",
    "        dff[\"snow_var_7d\"] = dff[\"snow_depth_cm\"].replace(-1, 0).shift(1).rolling(7).var().round(2)\n",
    "        dff[\"min_temp_2d\"] = dff[\"min_temp\"].shift(1).rolling(2).min().round(2)\n",
    "        dff[\"max_temp_2d\"] = dff[\"max_temp\"].shift(1).rolling(2).max().round(2)\n",
    "        dff[\"is_neg\"] = dff[\"min_temp\"]<0\n",
    "        dff[\"is_neg\"] = dff[\"is_neg\"].astype(int).shift(1)\n",
    "        dff[\"neg_rate_7d\"] = dff[\"is_neg\"].shift(1).rolling(7).mean().round(2)\n",
    "        dff = dff.bfill()\n",
    "        features = dff.drop(columns=['rainfall_mm', 'snow_depth_cm', 'air_temp', 'max_temp', 'min_temp', 'min_ground_temp', 'is_neg']).sort_index(axis=0, ascending=False)\n",
    "        features = features.head(1).fillna(0)\n",
    "        appended_data.append(features)\n",
    "    \n",
    "    all_features = pd.concat(appended_data)\n",
    "\n",
    "    return all_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Helsinki' 13.75 0.0 -0.62 7.78 0.14]\n",
      " ['Kuopio' 0.75 0.0 -3.67 3.3 0.29]\n",
      " ['Jyväskylä' 3.2 0.0 -7.25 4.15 0.57]\n",
      " ['Lahti' 1.9 0.0 -5.0 4.8 0.43]\n",
      " ['Oulu' 0.6 0.0 -5.37 4.4 0.29]]\n"
     ]
    }
   ],
   "source": [
    "# This part extracts features from the observation data. It returns a numpy array.\n",
    "# import model.weather_feature_extraction as m\n",
    "import datetime as dt\n",
    "# importlib.reload(m)\n",
    "\n",
    "df3 = get_features(df2, cities)\n",
    "X_pred = df3.to_numpy()\n",
    "date = df3.head(1).index.item() + dt.timedelta(days=1)\n",
    "\n",
    "print(X_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['10/27/2022' 'Helsinki' '0.0' '0.0']\n",
      " ['10/27/2022' 'Kuopio' '0.0' '0.0']\n",
      " ['10/27/2022' 'Jyväskylä' '0.0' '0.01']\n",
      " ['10/27/2022' 'Lahti' '0.0' '0.026946068012752393']\n",
      " ['10/27/2022' 'Oulu' '0.0' '0.01']]\n"
     ]
    }
   ],
   "source": [
    "# This part downloads the trained model and does the prediction for each city. \n",
    "# The output is a numpy array with Date, City, Binary prediction, Prediction as probability\n",
    "import joblib\n",
    "\n",
    "model = joblib.load('../model/random_forest_model')\n",
    "# model = joblib.load('./model/random_forest_model')\n",
    "\n",
    "predictions = []\n",
    "for i, city in enumerate(cities):\n",
    "    single_pred = []\n",
    "    single_pred.append(date.strftime(\"%m/%d/%Y\"))\n",
    "    single_pred.append(city)\n",
    "    y_pred = model.predict(X_pred[i,1:].reshape(1,5))\n",
    "    y_prob = model.predict_proba(X_pred[i,1:].reshape(1,5))\n",
    "    single_pred.append(y_pred[0])\n",
    "    single_pred.append(y_prob[0][1])\n",
    "    predictions.append(single_pred)\n",
    "\n",
    "predictions = np.array(predictions)\n",
    "\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-415a43ae6d46>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mshapefile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'../finland_shapefile/fi_1km.shp'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mfinMap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshapefile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m data = {'City': ['Helsinki', 'Kuopio', 'Jyväskylä', 'Lahti', 'Oulu'],\n\u001b[1;32m      9\u001b[0m            \u001b[0;34m'Latitude'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m60.17332\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m62.897968\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m62.242603\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m60.980381\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m65.012093\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/geopandas/io/file.py\u001b[0m in \u001b[0;36m_read_file\u001b[0;34m(filename, bbox, mask, rows, engine, **kwargs)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"fiona\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m         return _read_file_fiona(\n\u001b[0m\u001b[1;32m    260\u001b[0m             \u001b[0mpath_or_bytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrom_bytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbbox\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         )\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/geopandas/io/file.py\u001b[0m in \u001b[0;36m_read_file_fiona\u001b[0;34m(path_or_bytes, from_bytes, bbox, mask, rows, where, **kwargs)\u001b[0m\n\u001b[1;32m    358\u001b[0m                 )\n\u001b[1;32m    359\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 360\u001b[0;31m                 df = GeoDataFrame.from_features(\n\u001b[0m\u001b[1;32m    361\u001b[0m                     \u001b[0mf_filt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"geometry\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m                 )\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/geopandas/geodataframe.py\u001b[0m in \u001b[0;36mfrom_features\u001b[0;34m(cls, features, crs, columns)\u001b[0m\n\u001b[1;32m    633\u001b[0m                 \u001b[0mfeature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__geo_interface__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m             row = {\n\u001b[0;32m--> 635\u001b[0;31m                 \u001b[0;34m\"geometry\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"geometry\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mfeature\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"geometry\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m             }\n\u001b[1;32m    637\u001b[0m             \u001b[0;31m# load properties\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/shapely/geometry/geo.py\u001b[0m in \u001b[0;36mshape\u001b[0;34m(context)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mLinearRing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mob\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"coordinates\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mgeom_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"polygon\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mPolygon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mob\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"coordinates\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mob\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"coordinates\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mgeom_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"multipoint\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mMultiPoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mob\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"coordinates\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/shapely/geometry/polygon.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, shell, holes)\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mshell\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgeos_polygon_from_py\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mholes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m                 \u001b[0mgeom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/shapely/geometry/polygon.py\u001b[0m in \u001b[0;36mgeos_polygon_from_py\u001b[0;34m(shell, holes)\u001b[0m\n\u001b[1;32m    567\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m         return (\n\u001b[0;32m--> 569\u001b[0;31m             lgeos.GEOSGeom_createPolygon(\n\u001b[0m\u001b[1;32m    570\u001b[0m                 c_void_p(geos_shell), geos_holes, L), ndim)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "\n",
    "# Reading shapefile of map of Finland and creating df for coordinates of cities.\n",
    "# shapefile = 'slipskip/finland_shapefile/fi_1km.shp'\n",
    "shapefile = '../finland_shapefile/fi_1km.shp'\n",
    "\n",
    "finMap = gpd.read_file(shapefile)\n",
    "data = {'City': ['Helsinki', 'Kuopio', 'Jyväskylä', 'Lahti', 'Oulu'],\n",
    "           'Latitude': [60.17332, 62.897968, 62.242603, 60.980381, 65.012093],\n",
    "           'Longitude': [24.94102, 27.678171, 25.747257, 25.654988, 25.465076]}\n",
    "\n",
    "longLat = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        City   Latitude  Longitude                   geometry  value\n",
      "0   Helsinki  60.173320  24.941020  POINT (24.94102 60.17332)      0\n",
      "1     Kuopio  62.897968  27.678171  POINT (27.67817 62.89797)      0\n",
      "2  Jyväskylä  62.242603  25.747257  POINT (25.74726 62.24260)      0\n",
      "3      Lahti  60.980381  25.654988  POINT (25.65499 60.98038)      0\n",
      "4       Oulu  65.012093  25.465076  POINT (25.46508 65.01209)      0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sofvanh/.local/lib/python3.8/site-packages/pyproj/crs/crs.py:141: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6\n",
      "  in_crs_string = _prepare_from_proj_string(in_crs_string)\n"
     ]
    }
   ],
   "source": [
    "from shapely.geometry import Point, Polygon\n",
    "\n",
    "# Creating map of Finland\n",
    "crs = {'init':'EPSG:4326'}\n",
    "geometry = [Point(xy) for xy in zip(longLat['Longitude'], longLat['Latitude'])]\n",
    "geo_df = gpd.GeoDataFrame(longLat, \n",
    "                          crs = crs, \n",
    "                          geometry = geometry)\n",
    "\n",
    "\n",
    "pred = pd.DataFrame(predictions)\n",
    "# print(pred[2])\n",
    "\n",
    "# Putting slip warning to the cities not covered yet\n",
    "geo_df['value'] = pred[2].apply(pd.to_numeric).astype(int)\n",
    "\n",
    "# Putting predicted value to Helsinki.\n",
    "# geo_df.loc[0,'value'] = y_pred\n",
    "print(geo_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    green\n",
      "1    green\n",
      "2    green\n",
      "3    green\n",
      "4    green\n",
      "Name: color, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Creating color column\n",
    "geo_df['color'] = geo_df['value'].mask(geo_df['value'] == 1, 'orange')\n",
    "geo_df['color'] = geo_df['color'].mask(geo_df['color'] == 0, 'green')\n",
    "print(geo_df['color'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sre_constants import GROUPREF_UNI_IGNORE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# print(geo_df)\n",
    "\n",
    "# Create map, green if no warning, orange if warning\n",
    "fig, ax = plt.subplots(figsize = (10,10))\n",
    "finMap.to_crs(epsg=4326).plot(ax=ax, color='lightgrey')\n",
    "geo_df.plot(ax=ax,c=geo_df['color'], marker='^', markersize=450)\n",
    "ax.annotate('Helsinki', xy=(24.94102, 60.17332), xytext=(3, 3), textcoords=\"offset points\")\n",
    "for x, y, label in zip(geo_df[\"Longitude\"], geo_df[\"Latitude\"], geo_df[\"City\"]):\n",
    "    ax.annotate(label, xy=(x, y), xytext=(3, 3), textcoords=\"offset points\")\n",
    "ax.set_title('Slip warnings in Finland')\n",
    "\n",
    "plt.savefig('../figures/' + dt.datetime.now().strftime(\"%Y-%m-%d\") + '.png', bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
